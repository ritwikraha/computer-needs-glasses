{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/computer-needs-glasses/blob/master/image-generation/%5BEXP%5D_SDXL_DreamBooth_LoRA_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUxRrLfLMnBb"
      },
      "outputs": [],
      "source": [
        "# Check the GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJXRjGZa1vFZ"
      },
      "source": [
        "Make sure to install `diffusers` from `main` .Download diffusers SDXL DreamBooth training script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55U57alsRIek"
      },
      "outputs": [],
      "source": [
        "# Install dependencies.\n",
        "!pip install bitsandbytes transformers accelerate peft -q\n",
        "!pip install git+https://github.com/huggingface/diffusers.git -q\n",
        "!pip install datasets -q\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "from google.colab import files\n",
        "from huggingface_hub import snapshot_download\n",
        "from PIL import Image\n",
        "import glob\n",
        "import requests\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "import json\n",
        "import locale\n",
        "from huggingface_hub import whoami\n",
        "from pathlib import Path\n",
        "\n",
        "from train_dreambooth_lora_sdxl import save_model_card\n",
        "from huggingface_hub import upload_folder, create_repo\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "bXNjNAlSBZWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "UP_0OSiDB4TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBcvYAyS8bU"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UNkbXwj126Q"
      },
      "source": [
        "**Let's get our training data!**\n",
        "For this example, we'll download some images from the hub\n",
        "\n",
        "If you already have a dataset on the hub you wish to use, you can skip this part and go straight to: \"Prep for\n",
        "training üíª\" section, where you'll simply specify the dataset name.\n",
        "\n",
        "If your images are saved locally, and/or you want to add BLIP generated captions,\n",
        "pick option 1 or 2 below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqqhZ9R9x3mG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# pick a name for the image folder\n",
        "local_dir = \"./dog/\"\n",
        "os.makedirs(local_dir)\n",
        "os.chdir(local_dir)\n",
        "\n",
        "# choose and upload local images into the newly created directory\n",
        "uploaded_images = files.upload()\n",
        "os.chdir(\"/content\") # back to parent directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piJkbOe9OZbX"
      },
      "source": [
        "Preview the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0R-hByAPkIg"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols, resize=256):\n",
        "\n",
        "    if resize is not None:\n",
        "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "# change path to display images from your local dir\n",
        "img_paths = \"./dog/*.jpeg\"\n",
        "imgs = [Image.open(path) for path in glob.glob(img_paths)]\n",
        "\n",
        "num_imgs_to_preview = 5\n",
        "image_grid(imgs[:num_imgs_to_preview], 1, num_imgs_to_preview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqrxnDMUTADH"
      },
      "source": [
        "### Generate custom captions with BLIP\n",
        "Load BLIP to auto caption your images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZK1QuLqx3mH"
      },
      "outputs": [],
      "source": [
        "# load the processor and the captioning model\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\",torch_dtype=torch.float16).to(device)\n",
        "\n",
        "# captioning utility\n",
        "def caption_images(input_image):\n",
        "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    pixel_values = inputs.pixel_values\n",
        "\n",
        "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfkwE439x3mH"
      },
      "outputs": [],
      "source": [
        "# create a list of (Pil.Image, path) pairs\n",
        "local_dir = \"./dog/\"\n",
        "imgs_and_paths = [(path,Image.open(path)) for path in glob.glob(f\"{local_dir}*.jpeg\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V9cYeVDx3mH"
      },
      "source": [
        "Now let's add the concept token identifier (e.g. TOK) to each caption using a caption prefix.\n",
        "Feel free to change the prefix according to the concept you're training on!\n",
        "- for this example we can use \"a photo of TOK,\" other options include:\n",
        "    - For styles - \"In the style of TOK\"\n",
        "    - For faces - \"photo of a TOK person\"\n",
        "- You can add additional identifiers to the prefix that can help steer the model in the right direction.\n",
        "-- e.g. for this example, instead of \"a photo of TOK\" we can use \"a photo of TOK dog\" / \"a photo of TOK corgi dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJcZyeSVx3mH"
      },
      "outputs": [],
      "source": [
        "caption_prefix = \"a photo of TOK dog, \"\n",
        "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
        "  for img in imgs_and_paths:\n",
        "      caption = caption_prefix + caption_images(img[1]).split(\"\\n\")[0]\n",
        "      entry = {\"file_name\":img[0].split(\"/\")[-1], \"prompt\": caption}\n",
        "      json.dump(entry, outfile)\n",
        "      outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_5Skm_cx3mI"
      },
      "source": [
        "Free some memory for the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4wfEW_cx3mI"
      },
      "outputs": [],
      "source": [
        "# delete the BLIP pipelines and free up some memory\n",
        "del blip_processor, blip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLptJV1vx3mI"
      },
      "source": [
        "## Prep for training üíª\n",
        "\n",
        "Initialize `accelerate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW7eFPl4eYXy"
      },
      "outputs": [],
      "source": [
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc0bxTr918es"
      },
      "source": [
        "### Log into your Hugging Face account\n",
        "Pass [your **write** access token](https://huggingface.co/settings/tokens) so that we can push the trained checkpoints to the Hugging Face Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Hf07sapecFY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-p7R70THc5"
      },
      "source": [
        "## Train! üî¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az-IQG-Ax3mJ"
      },
      "source": [
        "#### Set Hyperparameters ‚ö°\n",
        "To ensure we can DreamBooth with LoRA on a heavy pipeline like Stable Diffusion XL, we're using:\n",
        "\n",
        "* Gradient checkpointing (`--gradient_accumulation_steps`)\n",
        "* 8-bit Adam (`--use_8bit_adam`)\n",
        "* Mixed-precision training (`--mixed-precision=\"fp16\"`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZbz4IXe0bAn"
      },
      "source": [
        "### Launch training üöÄüöÄüöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIMwkmL7N82R"
      },
      "source": [
        "To allow for custom captions we need to install the `datasets` library, you can skip that if you want to train solely\n",
        " with `--instance_prompt`.\n",
        "In that case, specify `--instance_data_dir` instead of `--dataset_name`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Use `--output_dir` to specify your LoRA model repository name!\n",
        " - Use `--caption_column` to specify name of the cpation column in your dataset. In this example we used \"prompt\" to\n",
        " save our captions in the\n",
        " metadata file, change this according to your needs."
      ],
      "metadata": {
        "collapsed": false,
        "id": "fbri7qQ0mdyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOiZSXHfx3mJ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env bash\n",
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"dog\" \\\n",
        "  --output_dir=\"corgy_dog_LoRA\" \\\n",
        "  --caption_column=\"prompt\"\\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --instance_prompt=\"a photo of TOK dog\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=3 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --max_train_steps=500 \\\n",
        "  --checkpointing_steps=717 \\\n",
        "  --seed=\"0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11tgWz5x3mK"
      },
      "source": [
        "### Save your model to the hub and check it out üî•\n",
        "\n",
        "make sure the `output_dir` you specify here is the same as the one used for training\n",
        "\n",
        "Sometimes training finishes succesfuly (i.e. a **.safetensores** file with the LoRA weights saved properly to your local `output_dir`) but there's not enough RAM in the free tier to push the model to the hub üôÅ\n",
        "To mitigate this, run this cell with your training arguments to make sure your model is uploaded! ü§ó\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uyy9n54EtBY"
      },
      "outputs": [],
      "source": [
        "output_dir = \"corgy_dog_LoRA\"\n",
        "username = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\n",
        "repo_id = f\"{username}/{output_dir}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smLheRkvEQ9H"
      },
      "outputs": [],
      "source": [
        "# push to the hubüî•\n",
        "repo_id = create_repo(repo_id, exist_ok=True).repo_id\n",
        "\n",
        "# change the params below according to your training arguments\n",
        "save_model_card(\n",
        "    repo_id = repo_id,\n",
        "    images=[],\n",
        "    base_model=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    train_text_encoder=False,\n",
        "    instance_prompt=\"a photo of TOK dog\",\n",
        "    validation_prompt=None,\n",
        "    repo_folder=output_dir,\n",
        "    vae_path=\"madebyollin/sdxl-vae-fp16-fix\",\n",
        ")\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=output_dir,\n",
        "    commit_message=\"End of training\",\n",
        "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLoRqZGux3mK"
      },
      "outputs": [],
      "source": [
        "link_to_model = f\"https://huggingface.co/{repo_id}\"\n",
        "display(Markdown(\"### Your model has finished training.\\nAccess it here: {}\".format(link_to_model)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvu9e6q_x3mK"
      },
      "source": [
        "Let's generate some images with it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH7-YJwMcyra"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTz6Zmfc0i-0"
      },
      "outputs": [],
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ")\n",
        "pipe.load_lora_weights(repo_id)\n",
        "_ = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J1NsUP51E2w"
      },
      "outputs": [],
      "source": [
        "prompt = \"a photo of TOK dog in a bucket at the beach\"\n",
        "\n",
        "image = pipe(prompt=prompt, num_inference_steps=50).images[0]\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}